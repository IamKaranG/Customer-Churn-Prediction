
## BUSINESS PROBLEM ##

# Business problem is very simple, 
# There is a Bank who wants to
# retain their existing customers who may churn, for that they have to
# know which existing customers has the highest probability of
# leaving the company and which customers have lowest probability
# of leaving the company.
# Churning of customers is a big problem for banks, they want to
# maximise the retention of the customers so that they can plan their
# future projects.
# Also if a Bank wants to get a funding from big investors, Rate of
# churning plays an important role.
# So Bank wants to build a Machine Learning model which can
# predict whether a particular customer will churn or not. 


## DATA ##
# We have 10000 rows with 13 features and 1 target variable, Every
# row tells us different customer details and whether they have
# churned or not.
# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
# importing the data
df=pd.read_csv("C:/Users/Desktop/ML_Project/Customer_Churn_Prediction/CSV_Folder/Churn_Modelling.csv")
# checking the data in file
df
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
0	1	15634602	Hargrave	619	France	Female	42	2	0.00	1	1	1	101348.88	1
1	2	15647311	Hill	608	Spain	Female	41	1	83807.86	1	0	1	112542.58	0
2	3	15619304	Onio	502	France	Female	42	8	159660.80	3	1	0	113931.57	1
3	4	15701354	Boni	699	France	Female	39	1	0.00	2	0	0	93826.63	0
4	5	15737888	Mitchell	850	Spain	Female	43	2	125510.82	1	1	1	79084.10	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
9995	9996	15606229	Obijiaku	771	France	Male	39	5	0.00	2	1	0	96270.64	0
9996	9997	15569892	Johnstone	516	France	Male	35	10	57369.61	1	1	1	101699.77	0
9997	9998	15584532	Liu	709	France	Female	36	7	0.00	1	0	1	42085.58	1
9998	9999	15682355	Sabbatini	772	Germany	Male	42	3	75075.31	2	1	0	92888.52	1
9999	10000	15628319	Walker	792	France	Female	28	4	130142.79	1	1	0	38190.78	0
10000 rows × 14 columns

Observations
# checking the shape of data
df.shape
(10000, 14)
# now we'll check whether there are any missing values in it or not ?
df.isnull().sum()
RowNumber          0
CustomerId         0
Surname            0
CreditScore        0
Geography          0
Gender             0
Age                0
Tenure             0
Balance            0
NumOfProducts      0
HasCrCard          0
IsActiveMember     0
EstimatedSalary    0
Exited             0
dtype: int64
# now checking for the duplicate data
df.duplicated().sum()
0
# let say if we had duplicate data then we would have displayed the duplicate data in following way
duplicate = df[df.duplicated()] # it will return the rows with duplicate entries
duplicate.head()
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
# now checking for the numbers of customers that churned or those who have not churned
df["Exited"].value_counts()  # here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
0    7963
1    2037
Name: Exited, dtype: int64
# fetching all the columns
df.columns
Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',
       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',
       'IsActiveMember', 'EstimatedSalary', 'Exited'],
      dtype='object')
# now diving the data into Numerical and Categorical 
# here rownumber, customerId, surname is of no use for us
# now creating a dataframe that consists of only numerical data along with Target Variable
df_numerical = df[['CreditScore','Age','Balance','EstimatedSalary','Exited']]
df_numerical.head()

# here we are taking exited column in numerical is because we want to findout the relationship between the target variable and 
# other input variables
CreditScore	Age	Balance	EstimatedSalary	Exited
0	619	42	0.00	101348.88	1
1	608	41	83807.86	112542.58	0
2	502	42	159660.80	113931.57	1
3	699	39	0.00	93826.63	0
4	850	43	125510.82	79084.10	0
# now creating a dataframe that consists of only categorical data along with Target Variable
df_categorical = df[['Geography','Gender','Tenure','NumOfProducts', 'HasCrCard','IsActiveMember','Exited']]
df_categorical.head()

# here the reason why we took Tenure, NumOfProducts in categorical data is because they have finite categories in it
Geography	Gender	Tenure	NumOfProducts	HasCrCard	IsActiveMember	Exited
0	France	Female	2	1	1	1	1
1	Spain	Female	1	1	0	1	0
2	France	Female	8	3	1	0	1
3	France	Female	1	2	0	0	0
4	Spain	Female	2	1	1	1	0
EDA(Exploratory Data Analysis)
# here we will be doing following type of Analysis
# 1.Univariate, 2.Bivariate 
Univariate Analysis
1.CreditScore
# plotting the HISTOGRAM
# histogram gives us the frequency over interval
# y axis=> frequency, x axis=> numerical data
plt.hist(df_numerical['CreditScore'])
(array([  19.,  166.,  447.,  958., 1444., 1866., 1952., 1525.,  968.,
         655.]),
 array([350., 400., 450., 500., 550., 600., 650., 700., 750., 800., 850.]),
 <BarContainer object of 10 artists>)

## OBSERVATION:
# we can say by looking at the plot, that the maximum number of people have CreditScore ranging between 600 to 700
# and there are very few number of people that have CreditScore less than 500 and more than 800
# now plotting an DISTRIBUTION PLOT
sns.set_style("whitegrid")
sns.FacetGrid(df_numerical,hue="Exited",height=5).map(sns.distplot,'CreditScore').add_legend()
plt.show()

## OBSERVATION:
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
# PDF(Probability Density Function) of the Creditscore is not helpful to differentiate between the customers who churned and 
# who didn't churned. 
# now plotting the BOX PLOT
sns.boxplot(x = 'Exited',y = "CreditScore", data = df_numerical)
plt.show()

## OBSERVATION:
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
# 1. 25% (percent) of people who stayed have CreditScore less than equal to 580 and people who churned have CreditScore less than
#    equal to 570
# 2. 50% (percent) of people who stayed have CreditScore less than equal to 650 and people who churned have CreditScore less than
#    equal to 640
# 3. 75% (percent) of people who stayed have CreditScore less than equal to 720 and people who churned have CreditScore less than
#    equal to 710
## And as per whisker showing in the plot the people who have churned have CreditScore less than 400.
# now plotting VIOLIN PLOT
sns.violinplot(x = 'Exited', y = 'CreditScore', data = df_numerical)
plt.show()

## OBSERVATION:
# Violin plot is combination of Boxplot and PDF(Probability Distribution Frequency)
# Here the violin plot is not helping in analysis
2.Age
# plotting HISTOGRAM 
plt.hist(df_numerical["Age"])
(array([ 611., 2179., 3629., 1871.,  828.,  523.,  208.,  127.,   20.,
           4.]),
 array([18. , 25.4, 32.8, 40.2, 47.6, 55. , 62.4, 69.8, 77.2, 84.6, 92. ]),
 <BarContainer object of 10 artists>)

## OBSERVATION:
# here we can see that the graph is right skewed distribution curve.
# by looking at the plot we can say that maximum customers of bank are in age between 30 to 40
# and very few customers are above age of 60
# now plotting DISTRIBUTION PLOT
sns.set_style("whitegrid")
sns.FacetGrid(df_numerical,hue="Exited",height=5).map(sns.distplot,'Age').add_legend()
plt.show()

## OBSERVATION:
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
# here by looking at the plot we can say that people who tend to stay with bank are in age between 25 to 40
# and the people who have churned are between the age 40 to 55
# now BOXPLOT
sns.boxplot(x = "Exited", y = 'Age', data = df_numerical)
plt.show()

## OBSERVATION:
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
# 1. 25%(percent) of people how have not churned are of age less than equal to 31 and the people who have churned are of age less
#   than equal to 38.
# 2. 50%(percent) of people how have not churned are of age less than equal to 37 and the people who have churned are of age less
#   than equal to 46.
# 3. 75%(percent) of people how have not churned are of age less than equal to 42 and the people who have churned are of age less
#   than equal to 52.
## Customers who stayed and who left are overplapping around age range between 37 to 42
# now VIOLIN PLOT
sns.violinplot(x = 'Exited', y = 'Age', data = df_numerical)
plt.show()

## OBSERVATION:
# Here violin plot is not helping in analysing data.
3.Balance
# now plotting HISTOGRAM
plt.hist(df_numerical["Balance"])
plt.show()

## OBSERVATION:
# here we can see that maximum people have balace ranging from 0 to 25000
# and there is also good amount of people who have account balance between 1,00,000 to 1,50,000
# and there are very few people who have bank account balance above 1,50,000
# now DISTRIBUTION PLOT
sns.set_style("whitegrid")
sns.FacetGrid(df_numerical,hue='Exited',height=5).map(sns.distplot,'Balance').add_legend()
plt.show()

## OBSERVATION:
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
# Majority of the customers have zero balance in there account
# here we can also see that maximum number of people who have churned have zero balance
# but on other hand there are is also a good number of people with zero balance but they have not churned
# later the grapth is overlapping, with number of people churned is less as compared to number of people not churned
# now BOXPLOT
sns.boxplot(x = 'Exited', y = 'Balance', data = df_numerical)
plt.show()

## OBSERVATION
print(np.percentile(df_numerical["Balance"],36))
print(np.percentile(df_numerical["Balance"],37))
0.0
51854.097800000025
# here based on the above percentile we come to know that 36 percent of people have account balance less than equal to 0
# now VIOLIN PLOT
sns.violinplot(x = 'Exited', y = 'Balance', data = df_numerical)
plt.show()

## OBSERVATION:
# Here violin plot is not helping in analysing data.
4.Estimated Salary
# now plotting HISTOGRAM
sns.histplot(df_numerical["EstimatedSalary"])
plt.show()

## OBSERVATION:
# here there is not much difference between the number of customers in different salary range
# now DISTRIBUTION PLOT
sns.set_style("whitegrid")
sns.FacetGrid(df_numerical,hue='Exited',height=5).map(sns.distplot,'EstimatedSalary').add_legend()
plt.show()

## OBSERVATION:
# This plot does not help much
# now BOX PLOT
sns.boxplot(x='Exited', y='EstimatedSalary',data=df_numerical)
plt.show()

## OBSERVATIONS:
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
# 1. 25%(percent) of people how have not churned have estimated salary less than equal to 50000 and the people who have churned 
#    have estimated salary less than equal to 50000
# 2. 50%(percent) of people how have not churned have estimated salary less than equal to 100000 and the people who have churned 
#    have estimated salary less than equal to 100000
# 3. 75%(percent) of people how have not churned have estimated salary less than equal to 150000 and the people who have churned 
#    have estimated salary less than equal to 15000
# This plot is not helping much
Bivariate Analysis
# We will begine with the pair plot, to take a look inside the numerical features
sns.pairplot(df_numerical,hue='Exited')
plt.show()

## OBSERVATION:
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
# here when 2 columns are same, we cannot plot scatter plot. We get a PDF(Probability Density Function) as seen in above plot.
# CreditScore
# Age 
# Balance
# EstimatedSalary
# --------------- CreditScore | Age | Balace | EstimatedSalary
# Here we can see that there is a pattern forming between Age and CreditScore
# also there is a pattern forming between Age and EstimatedSalary
# now plotting for Age and CreditScore
# SCATTER PLOT
sns.set_style("whitegrid")
sns.FacetGrid(df_numerical,hue='Exited',height=5).map(plt.scatter,'Age','CreditScore').add_legend()
plt.show()

# SCATTER PLOT
sns.set_style("whitegrid")
sns.FacetGrid(df_numerical,hue='Exited',height=5).map(sns.scatterplot,'Age','EstimatedSalary').add_legend()
plt.show()

## OBSERVATION:
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
# here we can see that in center of plot a circle is forming consiting of people who have churned in maximum number
# and the maximum number of people churned belong to age group of 35 to 55 Approx
# so it means that people with age less than 35 and more than 55 more likely to stay with bank
Scaling of Numerical Features
# Here we will be doing scaling of the numerical features
# we will be converting then in range between 0 and 1, which is Min-Max Scaling or Normalization
df_numerical
CreditScore	Age	Balance	EstimatedSalary	Exited
0	619	42	0.00	101348.88	1
1	608	41	83807.86	112542.58	0
2	502	42	159660.80	113931.57	1
3	699	39	0.00	93826.63	0
4	850	43	125510.82	79084.10	0
...	...	...	...	...	...
9995	771	39	0.00	96270.64	0
9996	516	35	57369.61	101699.77	0
9997	709	36	0.00	42085.58	1
9998	772	42	75075.31	92888.52	1
9999	792	28	130142.79	38190.78	0
10000 rows × 5 columns

# importing the min-max scaler
from sklearn.preprocessing import MinMaxScaler
scaling=MinMaxScaler()
# local variables to store the min-maxed scaled data
credScore = scaling.fit_transform(df_numerical[['CreditScore']])
age = scaling.fit_transform(df_numerical[['Age']])
balance = scaling.fit_transform(df_numerical[['Balance']])
estimatedSalary = scaling.fit_transform(df_numerical[['EstimatedSalary']])
# now replacing the original columns with this updated column data which is min-max scaled in df_numerical
df_numerical['CreditScore']=credScore
df_numerical['Age']=age
df_numerical['Balance']=balance
df_numerical['EstimatedSalary']=estimatedSalary
df_numerical
CreditScore	Age	Balance	EstimatedSalary	Exited
0	0.538	0.324324	0.000000	0.506735	1
1	0.516	0.310811	0.334031	0.562709	0
2	0.304	0.324324	0.636357	0.569654	1
3	0.698	0.283784	0.000000	0.469120	0
4	1.000	0.337838	0.500246	0.395400	0
...	...	...	...	...	...
9995	0.842	0.283784	0.000000	0.481341	0
9996	0.332	0.229730	0.228657	0.508490	0
9997	0.718	0.243243	0.000000	0.210390	1
9998	0.844	0.324324	0.299226	0.464429	1
9999	0.884	0.135135	0.518708	0.190914	0
10000 rows × 5 columns

Categorical Features
df_categorical.columns
Index(['Geography', 'Gender', 'Tenure', 'NumOfProducts', 'HasCrCard',
       'IsActiveMember', 'Exited'],
      dtype='object')
df_categorical
Geography	Gender	Tenure	NumOfProducts	HasCrCard	IsActiveMember	Exited
0	France	Female	2	1	1	1	1
1	Spain	Female	1	1	0	1	0
2	France	Female	8	3	1	0	1
3	France	Female	1	2	0	0	0
4	Spain	Female	2	1	1	1	0
...	...	...	...	...	...	...	...
9995	France	Male	5	2	1	0	0
9996	France	Male	10	1	1	1	0
9997	France	Female	7	1	0	1	1
9998	Germany	Male	3	2	1	0	1
9999	France	Female	4	1	1	0	0
10000 rows × 7 columns

# now we will be using 1 HOT ENCODING Technique for converting the categorical data into numerical format 
# here we have 2 columns containing the categorical data; Geography and Gender
# creating a temperory dataframe with categorical columns
categorical = ['Geography','Gender']
df_categorical = pd.get_dummies(df_categorical,columns=categorical,drop_first=False)
df_categorical
Tenure	NumOfProducts	HasCrCard	IsActiveMember	Exited	Geography_France	Geography_Germany	Geography_Spain	Gender_Female	Gender_Male
0	2	1	1	1	1	1	0	0	1	0
1	1	1	0	1	0	0	0	1	1	0
2	8	3	1	0	1	1	0	0	1	0
3	1	2	0	0	0	1	0	0	1	0
4	2	1	1	1	0	0	0	1	1	0
...	...	...	...	...	...	...	...	...	...	...
9995	5	2	1	0	0	1	0	0	0	1
9996	10	1	1	1	0	1	0	0	0	1
9997	7	1	0	1	1	1	0	0	1	0
9998	3	2	1	0	1	0	1	0	0	1
9999	4	1	1	0	0	1	0	0	1	0
10000 rows × 10 columns

df_categorical.columns
Index(['Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Exited',
       'Geography_France', 'Geography_Germany', 'Geography_Spain',
       'Gender_Female', 'Gender_Male'],
      dtype='object')
1.Tenure
df_categorical["Tenure"]
0        2
1        1
2        8
3        1
4        2
        ..
9995     5
9996    10
9997     7
9998     3
9999     4
Name: Tenure, Length: 10000, dtype: int64
# here we are checking how many values are present in one category
temp_Tenure = df_categorical['Tenure'].value_counts()
temp_Tenure
2     1048
1     1035
7     1028
8     1025
5     1012
3     1009
4      989
9      984
6      967
10     490
0      413
Name: Tenure, dtype: int64
# now plotiing BARPLOT
sns.barplot(x=temp_Tenure.values,y=temp_Tenure.index)
plt.show()

## OBSERVATION:
# here we can see that maximum customers have tenures of 2 years while very less number of customers have tunure of 10 years
# now PIE CHART
plt.pie(temp_Tenure.values,labels=temp_Tenure.index, autopct='%1.1f%%')
plt.axis('equal')
(-1.0999999247712648,
 1.0999999964176792,
 -1.0999999394567594,
 1.099999963766278)

## OBSERVATION:
# here we can see that only 4.9 percent of people have tenure of 10 years and 4.1 percent of customers have tenure of 0 years
2.NumOfProducts
# here we are checking how many different cargeories are present here
temp_NumOfProducts = df_categorical["NumOfProducts"].value_counts()
temp_NumOfProducts
1    5084
2    4590
3     266
4      60
Name: NumOfProducts, dtype: int64
# now BARPLOT
sns.barplot(x=temp_NumOfProducts.values,y=temp_NumOfProducts.index)
plt.show()

## OBSERVATION:
# here by looking at the plot we can say that maximum customer have only 1 product
3.HasCrCard
temp_HasCrCard=df_categorical['HasCrCard'].value_counts()
temp_HasCrCard
1    7055
0    2945
Name: HasCrCard, dtype: int64
# now plotting the BARPLOT
sns.barplot(x=temp_HasCrCard.values, y=temp_HasCrCard.index)
plt.show()

## OBSERVATION:
# here we come to know that 2945 customers do not have creditcard while 7055 customers have credit card
# now we will split the main dataframe into 2 parts 
# churned(not stayed) and not_churned(stayed)
# after that we will find the mean, median for the nnumerical columns; CreditScore, Age, Balance, EstimatedSalary
# here 0 means => customers who stayed(not churned), 1 means => customers who left(churned)
not_churned = df[df['Exited']==0]
churned = df[df['Exited']==1]
not_churned
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
1	2	15647311	Hill	608	Spain	Female	41	1	83807.86	1	0	1	112542.58	0
3	4	15701354	Boni	699	France	Female	39	1	0.00	2	0	0	93826.63	0
4	5	15737888	Mitchell	850	Spain	Female	43	2	125510.82	1	1	1	79084.10	0
6	7	15592531	Bartlett	822	France	Male	50	7	0.00	2	1	1	10062.80	0
8	9	15792365	He	501	France	Male	44	4	142051.07	2	0	1	74940.50	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
9993	9994	15569266	Rahman	644	France	Male	28	7	155060.41	1	1	0	29179.52	0
9994	9995	15719294	Wood	800	France	Female	29	2	0.00	2	0	0	167773.55	0
9995	9996	15606229	Obijiaku	771	France	Male	39	5	0.00	2	1	0	96270.64	0
9996	9997	15569892	Johnstone	516	France	Male	35	10	57369.61	1	1	1	101699.77	0
9999	10000	15628319	Walker	792	France	Female	28	4	130142.79	1	1	0	38190.78	0
7963 rows × 14 columns

churned
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
0	1	15634602	Hargrave	619	France	Female	42	2	0.00	1	1	1	101348.88	1
2	3	15619304	Onio	502	France	Female	42	8	159660.80	3	1	0	113931.57	1
5	6	15574012	Chu	645	Spain	Male	44	8	113755.78	2	1	0	149756.71	1
7	8	15656148	Obinna	376	Germany	Female	29	4	115046.74	4	1	0	119346.88	1
16	17	15737452	Romeo	653	Germany	Male	58	1	132602.88	1	1	0	5097.67	1
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
9981	9982	15672754	Burbidge	498	Germany	Male	42	3	152039.70	1	1	1	53445.17	1
9982	9983	15768163	Griffin	655	Germany	Female	46	7	137145.12	1	1	0	115146.40	1
9991	9992	15769959	Ajuluchukwu	597	France	Female	53	4	88381.21	1	1	0	69384.71	1
9997	9998	15584532	Liu	709	France	Female	36	7	0.00	1	0	1	42085.58	1
9998	9999	15682355	Sabbatini	772	Germany	Male	42	3	75075.31	2	1	0	92888.52	1
2037 rows × 14 columns

#find the mean, median for the nnumerical columns; CreditScore, Age, Balance, EstimatedSalary of churned and not churned customers
## CREDITSCORE
print("Mean: ")
print(np.mean(not_churned['CreditScore']), " for customers who have not churned(stayed)")
print(np.mean(churned['CreditScore']), " for customers who have churned(not_stayed)")
Mean: 
651.8531960316463  for customers who have not churned(stayed)
645.3514972999509  for customers who have churned(not_stayed)
print("Median: ")
print(np.median(not_churned['CreditScore']), "for customers who have not churned(stayed)")
print(np.median(churned['CreditScore']), "for customers who have churned(not_stayed)")
Median: 
653.0 for customers who have not churned(stayed)
646.0 for customers who have churned(not_stayed)
print("Standard Deviation: ")
print(np.std(not_churned['CreditScore']), "for customers who have not churned(stayed)")
print(np.std(churned['CreditScore']), "for customers who have churned(not_stayed)")
Standard Deviation: 
95.64783071535247 for customers who have not churned(stayed)
100.29687481012597 for customers who have churned(not_stayed)
## AGE
print("Mean: ")
print(np.mean(not_churned["Age"]), "for customers who have not churned(stayed)")
print(np.mean(churned["Age"]), "for customers who have churned(not_stayed)")
Mean: 
37.40838879819164 for customers who have not churned(stayed)
44.8379970544919 for customers who have churned(not_stayed)
print("Median: ")
print(np.median(not_churned['Age']), "for customers who have not churned(stayed)")
print(np.median(churned['Age']), "for customers who have churned(not_stayed)")
Median: 
36.0 for customers who have not churned(stayed)
45.0 for customers who have churned(not_stayed)
print("Standard Deviation: ")
print(np.std(not_churned['Age']), "for customers who have not churned(stayed)")
print(np.std(churned['Age']), "for customers who have churned(not_stayed)")
Standard Deviation: 
10.124727115441777 for customers who have not churned(stayed)
9.759165198147958 for customers who have churned(not_stayed)
## BALANCE
print("Mean: ")
print(np.mean(not_churned["Balance"]), "for customers who have not churned(stayed)")
print(np.mean(churned["Balance"]), "for customers who have churned(not_stayed)")
Mean: 
72745.2967788522 for customers who have not churned(stayed)
91108.53933726068 for customers who have churned(not_stayed)
print("Median: ")
print(np.median(not_churned['Balance']), "for customers who have not churned(stayed)")
print(np.median(churned['Balance']), "for customers who have churned(not_stayed)")
Median: 
92072.68 for customers who have not churned(stayed)
109349.29 for customers who have churned(not_stayed)
print("Standard Deviation: ")
print(np.std(not_churned['Balance']), "for customers who have not churned(stayed)")
print(np.std(churned['Balance']), "for customers who have churned(not_stayed)")
Standard Deviation: 
62844.094322747915 for customers who have not churned(stayed)
58346.467874478956 for customers who have churned(not_stayed)
## OBSERVATION:
# here there is difference between the values of mean and median in balace, which means there are some outliers present 
## Now we will do the following 
# combining the numerical and categorical data
# splitting the data into training and testing
# training the models
# testing for the performance
# conclusion
# now combining both numerical and categorical data
# it it basically a horizontal join
# before that we will be removing the Exited column from the categorical dataframe as it is already in numerical and we don't want duplicates

df_categorical2 = df_categorical.drop("Exited",axis=1)
df_categorical2
Tenure	NumOfProducts	HasCrCard	IsActiveMember	Geography_France	Geography_Germany	Geography_Spain	Gender_Female	Gender_Male
0	2	1	1	1	1	0	0	1	0
1	1	1	0	1	0	0	1	1	0
2	8	3	1	0	1	0	0	1	0
3	1	2	0	0	1	0	0	1	0
4	2	1	1	1	0	0	1	1	0
...	...	...	...	...	...	...	...	...	...
9995	5	2	1	0	1	0	0	0	1
9996	10	1	1	1	1	0	0	0	1
9997	7	1	0	1	1	0	0	1	0
9998	3	2	1	0	0	1	0	0	1
9999	4	1	1	0	1	0	0	1	0
10000 rows × 9 columns

df_concat = pd.concat([df_categorical2, df_numerical],axis=1) # axis = 1 means we are joining 2 dataframes horizontally
df_concat
Tenure	NumOfProducts	HasCrCard	IsActiveMember	Geography_France	Geography_Germany	Geography_Spain	Gender_Female	Gender_Male	CreditScore	Age	Balance	EstimatedSalary	Exited
0	2	1	1	1	1	0	0	1	0	0.538	0.324324	0.000000	0.506735	1
1	1	1	0	1	0	0	1	1	0	0.516	0.310811	0.334031	0.562709	0
2	8	3	1	0	1	0	0	1	0	0.304	0.324324	0.636357	0.569654	1
3	1	2	0	0	1	0	0	1	0	0.698	0.283784	0.000000	0.469120	0
4	2	1	1	1	0	0	1	1	0	1.000	0.337838	0.500246	0.395400	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
9995	5	2	1	0	1	0	0	0	1	0.842	0.283784	0.000000	0.481341	0
9996	10	1	1	1	1	0	0	0	1	0.332	0.229730	0.228657	0.508490	0
9997	7	1	0	1	1	0	0	1	0	0.718	0.243243	0.000000	0.210390	1
9998	3	2	1	0	0	1	0	0	1	0.844	0.324324	0.299226	0.464429	1
9999	4	1	1	0	1	0	0	1	0	0.884	0.135135	0.518708	0.190914	0
10000 rows × 14 columns

# now we will label the data as x = inputs/features and y = outputs/class labels
x = df_concat.iloc[:,0:13]
x
Tenure	NumOfProducts	HasCrCard	IsActiveMember	Geography_France	Geography_Germany	Geography_Spain	Gender_Female	Gender_Male	CreditScore	Age	Balance	EstimatedSalary
0	2	1	1	1	1	0	0	1	0	0.538	0.324324	0.000000	0.506735
1	1	1	0	1	0	0	1	1	0	0.516	0.310811	0.334031	0.562709
2	8	3	1	0	1	0	0	1	0	0.304	0.324324	0.636357	0.569654
3	1	2	0	0	1	0	0	1	0	0.698	0.283784	0.000000	0.469120
4	2	1	1	1	0	0	1	1	0	1.000	0.337838	0.500246	0.395400
...	...	...	...	...	...	...	...	...	...	...	...	...	...
9995	5	2	1	0	1	0	0	0	1	0.842	0.283784	0.000000	0.481341
9996	10	1	1	1	1	0	0	0	1	0.332	0.229730	0.228657	0.508490
9997	7	1	0	1	1	0	0	1	0	0.718	0.243243	0.000000	0.210390
9998	3	2	1	0	0	1	0	0	1	0.844	0.324324	0.299226	0.464429
9999	4	1	1	0	1	0	0	1	0	0.884	0.135135	0.518708	0.190914
10000 rows × 13 columns

y = df_concat.iloc[:,13:]
y
Exited
0	1
1	0
2	1
3	0
4	0
...	...
9995	0
9996	0
9997	1
9998	1
9999	0
10000 rows × 1 columns

# now splitting the data into training and testing
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=21)
x_train.shape
(8000, 13)
x_test.shape
(2000, 13)
y_train.shape
(8000, 1)
y_test.shape
(2000, 1)
MODELLING
KNN(K Nearest Neighbour)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
# creating the KNN Classifier
hyperparameter_k=[{'n_neighbors':[3,5,7,9]}]
knn_Classifier = KNeighborsClassifier() # creating an instance of knn model
model_knn = GridSearchCV(knn_Classifier,hyperparameter_k,scoring='accuracy') # applying grid search method for the hyperparameter
# tuning along with cross validation to find the best combination of hyperparameters for a KNN classifier and it uses accuracy 
# as the metric to evaluate each combination of hyperparameters.
model_knn.fit(x_train,y_train)

print("knn model best estimator")
print(model_knn.best_estimator_)
print("training accuracy is",model_knn.score(x_train,y_train))
print("testing accuracy is ",model_knn.score(x_test,y_test))
knn model best estimator
KNeighborsClassifier(n_neighbors=9)
training accuracy is 0.83475
testing accuracy is  0.807
# now we will do PREDICTIONS
# here we are storing the predicitions done by the model in variables
knn_prediction_on_Training_Data = model_knn.predict(x_train)
knn_prediction_on_Testing_Data = model_knn.predict(x_test)
# now we will be EVALUATING the models performance
# we will be using the Performance matrix
# Accuracy, Precision, Recall, F1 Score, Confusion mtrix
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score
# CONFUSION MATRIX

# Confusion Matrix of model for training data
KNN_confusion_Matrix_Train = confusion_matrix(y_train,knn_prediction_on_Training_Data)
print(KNN_confusion_Matrix_Train)
[[6187  176]
 [1146  491]]
print("Confusion Matrix of model for training data")
sns.set(font_scale=1) # setting the default fontsize
sns.heatmap(KNN_confusion_Matrix_Train, annot=True, annot_kws={"size":27},fmt='g',cmap="YlGnBu")
Confusion Matrix of model for training data
<Axes: >

# Confusion Matrix of model for testing data
KNN_confusion_Matrix_Test = confusion_matrix(y_test,knn_prediction_on_Testing_Data)
print(KNN_confusion_Matrix_Test)
[[1524   76]
 [ 310   90]]
print("Confusion Matrix of model for testing data")
sns.set(font_scale=1)
sns.heatmap(KNN_confusion_Matrix_Test,annot=True,annot_kws={"size":27},fmt="g",cmap="YlGnBu")
Confusion Matrix of model for testing data
<Axes: >

# PRECISION

# Calculating the Precision of Model for Training Data
KNN_Precision_Train = precision_score(y_train, knn_prediction_on_Training_Data)
print("The Precision value for the Training Data is: ", KNN_Precision_Train)
The Precision value for the Training Data is:  0.7361319340329835
# Calculating the Precision of Model for Testing Data
KNN_Precision_Test = precision_score(y_test, knn_prediction_on_Testing_Data)
print("The Precision value for the Testing Data is: ", KNN_Precision_Test)
The Precision value for the Testing Data is:  0.5421686746987951
# RECALL

# Calculating the Recall of Model for Training data
KNN_Recall_Train = recall_score(y_train, knn_prediction_on_Training_Data)
print("The Recall value for the Training Data is: ", KNN_Recall_Train)
The Recall value for the Training Data is:  0.2999389126450825
# Calculating the Recall of Model for Testing data
KNN_Recall_Test = recall_score(y_test, knn_prediction_on_Testing_Data)
print("The Recall value for the Testing Data is: ", KNN_Recall_Test)
The Recall value for the Testing Data is:  0.225
# F1 Score

# calculating the f1 score of the model for the Training Data
KNN_f1Score_Training = f1_score(y_train,knn_prediction_on_Training_Data)
print("The F1 Score for the Training Data is: ", KNN_f1Score_Training)
The F1 Score for the Training Data is:  0.42621527777777785
# calculating the f1 score of the model for the Testing Data
KNN_f1Score_Testing = f1_score(y_test, knn_prediction_on_Testing_Data)
print("The F1 Score for the Testing Data is: ", KNN_f1Score_Testing)
The F1 Score for the Testing Data is:  0.31802120141342755
# ACCURACY

# calculating the accuracy for the model for training data
KNN_Accuracy_Train = accuracy_score(y_train, knn_prediction_on_Training_Data)
print("The Accuracy for the Training Data is: ", KNN_Accuracy_Train)
The Accuracy for the Training Data is:  0.83475
# calculating the accuracy for the model for Testing data
KNN_Accuracy_Test = accuracy_score(y_test, knn_prediction_on_Testing_Data)
print("The Accuracy for the Testing Data is: ", KNN_Accuracy_Test)
The Accuracy for the Testing Data is:  0.807
LOGISTIC REGRESSION
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
tuned_parameters=[{'C':[10**-4,10**-2,10**0,10**2,10**4]}]
LR_model=GridSearchCV(LogisticRegression(max_iter=400,class_weight='balanced'),tuned_parameters)
LR_model.fit(x_train,y_train)

print(LR_model.best_estimator_)
print(LR_model.score(x_train,y_train))
print(LR_model.score(x_test,y_test))
LogisticRegression(C=10000, class_weight='balanced', max_iter=400)
0.714375
0.7115
tuned_parameters=[{'C':[10**-4,10**-2,10**0,10**2,10**4,10**6,10**8]}]
LR1_model=GridSearchCV(LogisticRegression(max_iter=400,class_weight='balanced', penalty='l2'),tuned_parameters)
LR1_model.fit(x_train,y_train)

print(LR1_model.best_estimator_)
print(LR1_model.score(x_train,y_train))
print(LR1_model.score(x_test,y_test))
LogisticRegression(C=1000000, class_weight='balanced', max_iter=400)
0.714375
0.7115
 
